{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9736647",
   "metadata": {},
   "source": [
    "# Visual RAG with Knowledge Graph Demo\n",
    "\n",
    "This notebook demonstrates the new Visual RAG capabilities that extend the original RAG-knowledgegraph project with:\n",
    "\n",
    "1. **Multimodal Visual RAG**: Image ingestion, OCR, captioning, and visual question answering\n",
    "2. **Interactive Web UI**: Streamlit-based interface with knowledge graph visualization\n",
    "3. **Enhanced Provenance**: Detailed tracking of text and image sources\n",
    "\n",
    "## Features Demonstrated\n",
    "\n",
    "- üñºÔ∏è **Image Processing**: OCR text extraction, image captioning, object detection\n",
    "- üß† **Multimodal Knowledge Graph**: Combining text documents with visual information\n",
    "- üîç **Visual Question Answering**: Queries that leverage both text and image content\n",
    "- üìä **Interactive Visualization**: Knowledge graph visualization with provenance\n",
    "- üí¨ **Feedback Collection**: User feedback system for continuous improvement\n",
    "\n",
    "Let's start by setting up the environment and exploring these new capabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, KnowledgeGraphIndex, Settings, StorageContext\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Import our custom modules\n",
    "from pipeline.visual_rag import VisualRAGPipeline, VisualRAGResult\n",
    "from ingest.image_ingest import ImageIngestor\n",
    "from tools.knowledge_graph_visualizer import KnowledgeGraphVisualizer\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91170d0",
   "metadata": {},
   "source": [
    "## 1. Setup Configuration\n",
    "\n",
    "First, let's configure our models and set up the necessary tokens. You'll need a HuggingFace token for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb34b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "HF_TOKEN = 'your_huggingface_token_here'  # Replace with your HuggingFace token\n",
    "\n",
    "# Model configurations\n",
    "LLM_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "EMBEDDING_MODEL = \"thenlper/gte-large\"\n",
    "CAPTION_MODEL = \"Salesforce/blip-image-captioning-base\"\n",
    "\n",
    "# Paths\n",
    "DOCUMENTS_PATH = \"../documents/sample_ai\"\n",
    "IMAGES_PATH = \"../documents\"\n",
    "STORAGE_PATH = \"../storage\"\n",
    "\n",
    "# Display configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"üìÑ Documents path: {DOCUMENTS_PATH}\")\n",
    "print(f\"üñºÔ∏è Images path: {IMAGES_PATH}\")\n",
    "print(f\"üíæ Storage path: {STORAGE_PATH}\")\n",
    "\n",
    "# Check if paths exist\n",
    "if not os.path.exists(DOCUMENTS_PATH):\n",
    "    print(f\"‚ö†Ô∏è Warning: Documents path does not exist: {DOCUMENTS_PATH}\")\n",
    "if not os.path.exists(IMAGES_PATH):\n",
    "    print(f\"‚ö†Ô∏è Warning: Images path does not exist: {IMAGES_PATH}\")\n",
    "\n",
    "if HF_TOKEN == 'your_huggingface_token_here':\n",
    "    print(\"‚ö†Ô∏è Please set your HuggingFace token in the configuration above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fcb0a0",
   "metadata": {},
   "source": [
    "## 2. Initialize Models and Load Documents\n",
    "\n",
    "Let's set up our LLM, embedding model, and load the existing documents to create our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HF_TOKEN = os.environ.get('HUGGINGFACEHUB_API_TOKEN', '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')\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    model_kwargs={\"use_auth_token\": HF_TOKEN}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc22226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documents and Create Knowledge Graph\n",
    "print(\"üìö Loading documents and creating knowledge graph...\")\n",
    "\n",
    "try:\n",
    "    # Load documents\n",
    "    documents = SimpleDirectoryReader(DOCUMENTS_PATH).load_data()\n",
    "    print(f\"‚úÖ Loaded {len(documents)} documents\")\n",
    "    \n",
    "    # Display document information\n",
    "    for i, doc in enumerate(documents):\n",
    "        print(f\"  üìÑ Document {i+1}: {len(doc.text)} characters\")\n",
    "        if hasattr(doc, 'metadata') and 'file_name' in doc.metadata:\n",
    "            print(f\"     File: {doc.metadata['file_name']}\")\n",
    "    \n",
    "    # Setup storage context\n",
    "    graph_store = SimpleGraphStore()\n",
    "    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "    \n",
    "    # Create Knowledge Graph Index\n",
    "    print(\"üî® Building knowledge graph...\")\n",
    "    kg_index = KnowledgeGraphIndex.from_documents(\n",
    "        documents=documents,\n",
    "        max_triplets_per_chunk=3,\n",
    "        storage_context=storage_context,\n",
    "        embed_model=embed_model,\n",
    "        include_embeddings=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Knowledge graph created successfully!\")\n",
    "    \n",
    "    # Get basic statistics\n",
    "    graph = kg_index.get_networkx_graph()\n",
    "    print(f\"üìä Graph statistics:\")\n",
    "    print(f\"   ‚Ä¢ Nodes: {len(graph.nodes)}\")\n",
    "    print(f\"   ‚Ä¢ Edges: {len(graph.edges)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating knowledge graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ea743",
   "metadata": {},
   "source": [
    "## 3. Add Visual Capabilities\n",
    "\n",
    "Now let's initialize our Visual RAG pipeline and add images to the knowledge graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4353efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Visual RAG Pipeline\n",
    "print(\"üñºÔ∏è Initializing Visual RAG pipeline...\")\n",
    "\n",
    "try:\n",
    "    # Create Visual RAG Pipeline\n",
    "    visual_rag = VisualRAGPipeline(kg_index)\n",
    "    print(\"‚úÖ Visual RAG pipeline created\")\n",
    "    \n",
    "    # Initialize Image Ingestor\n",
    "    image_ingestor = ImageIngestor(\n",
    "        caption_model_name=CAPTION_MODEL\n",
    "    )\n",
    "    print(\"‚úÖ Image ingestor initialized\")\n",
    "    \n",
    "    # Find available images\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n",
    "    available_images = []\n",
    "    \n",
    "    if os.path.exists(IMAGES_PATH):\n",
    "        for filename in os.listdir(IMAGES_PATH):\n",
    "            if any(filename.lower().endswith(ext) for ext in image_extensions):\n",
    "                available_images.append(os.path.join(IMAGES_PATH, filename))\n",
    "    \n",
    "    print(f\"üîç Found {len(available_images)} images:\")\n",
    "    for img_path in available_images:\n",
    "        print(f\"   üì∏ {os.path.basename(img_path)}\")\n",
    "    \n",
    "    if available_images:\n",
    "        # Process images and add to knowledge graph\n",
    "        print(\"üî® Processing images and adding to knowledge graph...\")\n",
    "        num_nodes_added = visual_rag.add_images_to_kg(available_images)\n",
    "        print(f\"‚úÖ Added {num_nodes_added} image-related nodes to knowledge graph\")\n",
    "        \n",
    "        # Update graph statistics\n",
    "        updated_graph = kg_index.get_networkx_graph()\n",
    "        print(f\"üìä Updated graph statistics:\")\n",
    "        print(f\"   ‚Ä¢ Nodes: {len(updated_graph.nodes)} (+{len(updated_graph.nodes) - len(graph.nodes)})\")\n",
    "        print(f\"   ‚Ä¢ Edges: {len(updated_graph.edges)} (+{len(updated_graph.edges) - len(graph.edges)})\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No images found to process\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing visual capabilities: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579aa636",
   "metadata": {},
   "source": [
    "## 4. Test Multimodal Queries\n",
    "\n",
    "Now let's test our Visual RAG system with various types of queries that combine text and visual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a77cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries\n",
    "test_queries = [\n",
    "    \"What is artificial intelligence and how is it represented visually?\",\n",
    "    \"Who coined the term AI? Are there any images related to this topic?\",\n",
    "    \"Explain machine learning with any visual diagrams or charts available\",\n",
    "    \"What visual information is available about AI research?\",\n",
    "    \"How many PhD researchers are in India according to the documents and images?\"\n",
    "]\n",
    "\n",
    "print(\"üîç Testing Visual RAG with multimodal queries...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Test each query\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nüî∏ Query {i}: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Execute visual RAG query\n",
    "        result = visual_rag.query_with_visual_context(\n",
    "            query=query,\n",
    "            include_images=True,\n",
    "            max_text_results=3,\n",
    "            max_image_results=2\n",
    "        )\n",
    "        \n",
    "        results.append((query, result))\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"üí° Answer: {result.answer[:200]}...\")\n",
    "        print(f\"üìä Confidence: {result.confidence_score:.2%}\")\n",
    "        print(f\"üìÑ Text sources: {len(result.text_sources)}\")\n",
    "        print(f\"üñºÔ∏è Image sources: {len(result.image_sources)}\")\n",
    "        print(f\"üï∏Ô∏è Graph nodes: {len(result.graph_context.get('nodes', {}))}\")\n",
    "        \n",
    "        # Show source breakdown\n",
    "        if result.text_sources:\n",
    "            print(\"   Text sources:\")\n",
    "            for j, source in enumerate(result.text_sources[:2]):\n",
    "                preview = source['text'][:80] + \"...\" if len(source['text']) > 80 else source['text']\n",
    "                print(f\"     ‚Ä¢ {preview}\")\n",
    "        \n",
    "        if result.image_sources:\n",
    "            print(\"   Image sources:\")\n",
    "            for j, source in enumerate(result.image_sources[:2]):\n",
    "                img_type = source['metadata'].get('type', 'unknown')\n",
    "                print(f\"     ‚Ä¢ {img_type}: {source['text'][:60]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing query: {e}\")\n",
    "        results.append((query, None))\n",
    "\n",
    "print(f\"\\n‚úÖ Completed testing {len(test_queries)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca171fb",
   "metadata": {},
   "source": [
    "## 5. Visualize Results and Provenance\n",
    "\n",
    "Let's create visualizations to better understand our results and their provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Query Performance\n",
    "if results:\n",
    "    print(\"üìä Creating performance visualizations...\")\n",
    "    \n",
    "    # Extract metrics from results\n",
    "    query_data = []\n",
    "    for query, result in results:\n",
    "        if result:\n",
    "            query_data.append({\n",
    "                'Query': query[:30] + \"...\" if len(query) > 30 else query,\n",
    "                'Confidence': result.confidence_score,\n",
    "                'Text Sources': len(result.text_sources),\n",
    "                'Image Sources': len(result.image_sources),\n",
    "                'Graph Nodes': len(result.graph_context.get('nodes', {})),\n",
    "                'Total Sources': len(result.text_sources) + len(result.image_sources)\n",
    "            })\n",
    "    \n",
    "    if query_data:\n",
    "        df = pd.DataFrame(query_data)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Visual RAG Performance Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Confidence scores\n",
    "        ax1 = axes[0, 0]\n",
    "        bars1 = ax1.bar(range(len(df)), df['Confidence'], color='skyblue', alpha=0.7)\n",
    "        ax1.set_title('Confidence Scores by Query')\n",
    "        ax1.set_xlabel('Query Index')\n",
    "        ax1.set_ylabel('Confidence Score')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, bar in enumerate(bars1):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        # 2. Source distribution\n",
    "        ax2 = axes[0, 1]\n",
    "        source_data = df[['Text Sources', 'Image Sources']].sum()\n",
    "        ax2.pie(source_data, labels=source_data.index, autopct='%1.1f%%', \n",
    "                colors=['lightcoral', 'lightgreen'])\n",
    "        ax2.set_title('Distribution of Source Types')\n",
    "        \n",
    "        # 3. Sources per query\n",
    "        ax3 = axes[1, 0]\n",
    "        x = range(len(df))\n",
    "        width = 0.35\n",
    "        ax3.bar([i - width/2 for i in x], df['Text Sources'], width, \n",
    "                label='Text Sources', color='lightcoral', alpha=0.7)\n",
    "        ax3.bar([i + width/2 for i in x], df['Image Sources'], width,\n",
    "                label='Image Sources', color='lightgreen', alpha=0.7)\n",
    "        ax3.set_title('Sources per Query')\n",
    "        ax3.set_xlabel('Query Index')\n",
    "        ax3.set_ylabel('Number of Sources')\n",
    "        ax3.legend()\n",
    "        \n",
    "        # 4. Graph nodes vs confidence\n",
    "        ax4 = axes[1, 1]\n",
    "        scatter = ax4.scatter(df['Graph Nodes'], df['Confidence'], \n",
    "                             c=df['Total Sources'], cmap='viridis', alpha=0.7, s=100)\n",
    "        ax4.set_title('Graph Nodes vs Confidence')\n",
    "        ax4.set_xlabel('Number of Graph Nodes')\n",
    "        ax4.set_ylabel('Confidence Score')\n",
    "        plt.colorbar(scatter, ax=ax4, label='Total Sources')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display summary table\n",
    "        print(\"\\nüìã Query Results Summary:\")\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid results to visualize\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f914e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Analysis of Best Result\n",
    "if results:\n",
    "    # Find the result with highest confidence\n",
    "    valid_results = [(q, r) for q, r in results if r is not None]\n",
    "    \n",
    "    if valid_results:\n",
    "        best_query, best_result = max(valid_results, key=lambda x: x[1].confidence_score)\n",
    "        \n",
    "        print(f\"üèÜ Best Result Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üî∏ Query: {best_query}\")\n",
    "        print(f\"üìä Confidence: {best_result.confidence_score:.2%}\")\n",
    "        print(f\"üí° Answer: {best_result.answer}\")\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        \n",
    "        # Show detailed sources\n",
    "        print(f\"\\nüìÑ Text Sources ({len(best_result.text_sources)}):\")\n",
    "        for i, source in enumerate(best_result.text_sources, 1):\n",
    "            print(f\"   {i}. Score: {source.get('score', 0):.3f}\")\n",
    "            print(f\"      Text: {source['text'][:150]}...\")\n",
    "            print(f\"      Type: {source['metadata'].get('type', 'text')}\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"\\nüñºÔ∏è Image Sources ({len(best_result.image_sources)}):\")\n",
    "        for i, source in enumerate(best_result.image_sources, 1):\n",
    "            print(f\"   {i}. Score: {source.get('score', 0):.3f}\")\n",
    "            print(f\"      Text: {source['text'][:100]}...\")\n",
    "            print(f\"      Type: {source['metadata'].get('type', 'image')}\")\n",
    "            if 'image_path' in source['metadata']:\n",
    "                print(f\"      Image: {os.path.basename(source['metadata']['image_path'])}\")\n",
    "            print()\n",
    "        \n",
    "        # Show graph context\n",
    "        graph_context = best_result.graph_context\n",
    "        print(f\"\\nüï∏Ô∏è Knowledge Graph Context:\")\n",
    "        print(f\"   ‚Ä¢ Nodes in context: {len(graph_context.get('nodes', {}))}\")\n",
    "        print(f\"   ‚Ä¢ Edges in context: {len(graph_context.get('edges', []))}\")\n",
    "        print(f\"   ‚Ä¢ Total graph nodes: {graph_context.get('total_graph_nodes', 0)}\")\n",
    "        \n",
    "        # Show provenance\n",
    "        print(f\"\\nüîç Provenance Information:\")\n",
    "        provenance = best_result.provenance\n",
    "        print(f\"   ‚Ä¢ Text node IDs: {len(provenance.get('text_node_ids', []))}\")\n",
    "        print(f\"   ‚Ä¢ Image node IDs: {len(provenance.get('image_node_ids', []))}\")\n",
    "        print(f\"   ‚Ä¢ Graph nodes used: {len(provenance.get('graph_nodes', []))}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid results to analyze\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da926c6",
   "metadata": {},
   "source": [
    "## 6. Demonstrate Web UI Capabilities\n",
    "\n",
    "Let's show how to launch the interactive web interface for visual exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web UI Launch Instructions\n",
    "print(\"üåê Interactive Web UI\")\n",
    "print(\"=\" * 40)\n",
    "print()\n",
    "print(\"To launch the interactive Streamlit web interface:\")\n",
    "print()\n",
    "print(\"1. Open a terminal in the project root directory\")\n",
    "print(\"2. Run the following command:\")\n",
    "print()\n",
    "print(\"   streamlit run webapp/app.py\")\n",
    "print()\n",
    "print(\"3. The web interface will open in your browser with features:\")\n",
    "print(\"   ‚Ä¢ üîß Model configuration\")\n",
    "print(\"   ‚Ä¢ üîç Interactive querying\")\n",
    "print(\"   ‚Ä¢ üìä Real-time results visualization\")\n",
    "print(\"   ‚Ä¢ üï∏Ô∏è Knowledge graph exploration\")\n",
    "print(\"   ‚Ä¢ üí¨ Feedback collection\")\n",
    "print(\"   ‚Ä¢ üìà Performance analytics\")\n",
    "print()\n",
    "print(\"4. Configure your settings in the sidebar:\")\n",
    "print(\"   ‚Ä¢ Add your HuggingFace token\")\n",
    "print(\"   ‚Ä¢ Select models\")\n",
    "print(\"   ‚Ä¢ Set document and image paths\")\n",
    "print(\"   ‚Ä¢ Click 'Initialize System'\")\n",
    "print()\n",
    "print(\"5. Start querying with natural language!\")\n",
    "print()\n",
    "\n",
    "# Show sample web UI interactions\n",
    "sample_interactions = [\n",
    "    {\n",
    "        \"query\": \"What is artificial intelligence?\",\n",
    "        \"description\": \"Basic text-based query to test the system\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Show me visual information about AI\",\n",
    "        \"description\": \"Image-focused query to retrieve visual content\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain machine learning with diagrams\",\n",
    "        \"description\": \"Multimodal query combining text and visual sources\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What images are available in the knowledge base?\",\n",
    "        \"description\": \"Query to explore available visual content\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üí° Sample queries to try in the web interface:\")\n",
    "print(\"-\" * 50)\n",
    "for i, interaction in enumerate(sample_interactions, 1):\n",
    "    print(f\"{i}. Query: \\\"{interaction['query']}\\\"\")\n",
    "    print(f\"   Purpose: {interaction['description']}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚ú® The web interface provides:\")\n",
    "print(\"‚Ä¢ Real-time visualization of knowledge graph relationships\")\n",
    "print(\"‚Ä¢ Interactive feedback collection for continuous improvement\")\n",
    "print(\"‚Ä¢ Detailed provenance tracking for transparency\")\n",
    "print(\"‚Ä¢ Side-by-side comparison of text and image sources\")\n",
    "print(\"‚Ä¢ Performance metrics and confidence scoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463598a",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully explored the enhanced Visual RAG capabilities. Here's what we accomplished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Summary\n",
    "print(\"üéâ Visual RAG Demo Summary\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "achievements = [\n",
    "    \"‚úÖ Successfully integrated image processing capabilities\",\n",
    "    \"‚úÖ Created multimodal knowledge graph with text and visual nodes\",\n",
    "    \"‚úÖ Implemented OCR, captioning, and object detection\",\n",
    "    \"‚úÖ Built Visual RAG pipeline for multimodal queries\",\n",
    "    \"‚úÖ Demonstrated provenance tracking and visualization\",\n",
    "    \"‚úÖ Created interactive web interface with Streamlit\",\n",
    "    \"‚úÖ Tested various query types and analyzed performance\"\n",
    "]\n",
    "\n",
    "for achievement in achievements:\n",
    "    print(achievement)\n",
    "\n",
    "print()\n",
    "print(\"üìä Key Metrics from this Demo:\")\n",
    "if 'df' in locals() and not df.empty:\n",
    "    print(f\"   ‚Ä¢ Queries tested: {len(df)}\")\n",
    "    print(f\"   ‚Ä¢ Average confidence: {df['Confidence'].mean():.2%}\")\n",
    "    print(f\"   ‚Ä¢ Total sources used: {df['Total Sources'].sum()}\")\n",
    "    print(f\"   ‚Ä¢ Images processed: {len(available_images) if 'available_images' in locals() else 0}\")\n",
    "\n",
    "print()\n",
    "print(\"üöÄ Next Steps:\")\n",
    "next_steps = [\n",
    "    \"1. Experiment with more complex multimodal queries\",\n",
    "    \"2. Add more diverse image types (diagrams, charts, screenshots)\",\n",
    "    \"3. Fine-tune confidence scoring and relevance ranking\",\n",
    "    \"4. Implement advanced object detection models\",\n",
    "    \"5. Add support for video content\",\n",
    "    \"6. Enhance the web UI with more visualization options\",\n",
    "    \"7. Collect user feedback to improve the system\",\n",
    "    \"8. Scale to larger document and image collections\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(step)\n",
    "\n",
    "print()\n",
    "print(\"üí° Advanced Usage Ideas:\")\n",
    "advanced_ideas = [\n",
    "    \"‚Ä¢ Medical document analysis with X-ray/MRI integration\",\n",
    "    \"‚Ä¢ Technical manual understanding with diagram support\",\n",
    "    \"‚Ä¢ Research paper analysis with figure comprehension\",\n",
    "    \"‚Ä¢ Educational content with visual learning materials\",\n",
    "    \"‚Ä¢ Legal document review with evidence images\",\n",
    "    \"‚Ä¢ Scientific literature with data visualizations\"\n",
    "]\n",
    "\n",
    "for idea in advanced_ideas:\n",
    "    print(idea)\n",
    "\n",
    "print()\n",
    "print(\"üìö Files Created in this Extension:\")\n",
    "files_created = [\n",
    "    \"ingest/image_ingest.py - Image processing and node creation\",\n",
    "    \"pipeline/visual_rag.py - Multimodal RAG pipeline\",\n",
    "    \"webapp/app.py - Streamlit web interface\",\n",
    "    \"webapp/provenance.py - Provenance tracking system\",\n",
    "    \"webapp/feedback_sink.py - User feedback collection\",\n",
    "    \"tools/visual_utils.py - Image utilities and visualization\",\n",
    "    \"notebooks/visual_rag_demo.ipynb - This demonstration notebook\"\n",
    "]\n",
    "\n",
    "for file_info in files_created:\n",
    "    print(f\"   üìÑ {file_info}\")\n",
    "\n",
    "print()\n",
    "print(\"üéØ This enhanced RAG system now provides:\")\n",
    "print(\"   ‚Ä¢ Multimodal understanding (text + images)\")\n",
    "print(\"   ‚Ä¢ Interactive web interface\")\n",
    "print(\"   ‚Ä¢ Detailed provenance tracking\")\n",
    "print(\"   ‚Ä¢ User feedback collection\")\n",
    "print(\"   ‚Ä¢ Visual knowledge graph exploration\")\n",
    "print(\"   ‚Ä¢ Comprehensive testing and evaluation\")\n",
    "print()\n",
    "print(\"Ready to revolutionize your document understanding! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
