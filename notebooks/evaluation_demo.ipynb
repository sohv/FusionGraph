{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0ebe04",
   "metadata": {},
   "source": [
    "# FusionGraph Research-Grade Evaluation\n",
    "\n",
    "This notebook demonstrates comprehensive evaluation of the FusionGraph multimodal RAG system using three key research metrics:\n",
    "\n",
    "1. **Retrieval Quality Assessment** - NDCG@K, MRR, Precision@K\n",
    "2. **Factual Consistency & Grounding** - NLI-based verification, claim support\n",
    "3. **Multimodal Integration Quality** - Cross-modal alignment, OCR accuracy\n",
    "\n",
    "These metrics provide quantitative measures to assess system performance for research and production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf725c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import evaluation framework\n",
    "from evaluation.evaluation_framework import (\n",
    "    FusionGraphEvaluationSuite,\n",
    "    RetrievalQualityEvaluator, \n",
    "    FactualConsistencyEvaluator,\n",
    "    MultimodalGroundingEvaluator\n",
    ")\n",
    "\n",
    "print(\"üìä FusionGraph Evaluation Framework Loaded\")\n",
    "print(\"‚úÖ Ready for research-grade evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130eaa6f",
   "metadata": {},
   "source": [
    "## Test 1: Retrieval Quality Assessment\n",
    "\n",
    "Measures how well the system retrieves relevant documents using information retrieval metrics:\n",
    "- **NDCG@K**: Normalized Discounted Cumulative Gain at rank K\n",
    "- **MRR**: Mean Reciprocal Rank of first relevant result\n",
    "- **Precision@K**: Fraction of top-K results that are relevant\n",
    "- **Recall@K**: Fraction of relevant documents found in top-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3976b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retrieval quality evaluator\n",
    "retrieval_eval = RetrievalQualityEvaluator()\n",
    "\n",
    "# Sample retrieval results for demonstration\n",
    "sample_retrieval_results = [\n",
    "    {'source_id': 'ai_healthcare_2024', 'score': 0.92},\n",
    "    {'source_id': 'cv_deep_learning_2024', 'score': 0.75}, \n",
    "    {'source_id': 'knowledge_graphs_2024', 'score': 0.68},\n",
    "    {'source_id': 'nlp_llm_2024', 'score': 0.45}\n",
    "]\n",
    "\n",
    "# Evaluate retrieval quality\n",
    "retrieval_metrics = retrieval_eval.evaluate_retrieval_quality(\n",
    "    sample_retrieval_results, \n",
    "    query_id=\"q1\",  # \"How does AI help in medical diagnosis?\"\n",
    "    k_values=[1, 3, 5]\n",
    ")\n",
    "\n",
    "print(\"üîç Retrieval Quality Metrics:\")\n",
    "for metric, score in retrieval_metrics.items():\n",
    "    print(f\"   {metric}: {score:.3f}\")\n",
    "\n",
    "# Visualize retrieval metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# NDCG at different K values\n",
    "ndcg_metrics = {k: v for k, v in retrieval_metrics.items() if 'ndcg' in k}\n",
    "k_vals = [int(k.split('_')[-1]) for k in ndcg_metrics.keys()]\n",
    "ndcg_scores = list(ndcg_metrics.values())\n",
    "\n",
    "axes[0].bar(k_vals, ndcg_scores, color='skyblue', alpha=0.7)\n",
    "axes[0].set_xlabel('K (Top-K Results)')\n",
    "axes[0].set_ylabel('NDCG Score')\n",
    "axes[0].set_title('NDCG@K Scores')\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Precision at different K values\n",
    "prec_metrics = {k: v for k, v in retrieval_metrics.items() if 'precision' in k}\n",
    "prec_scores = list(prec_metrics.values())\n",
    "\n",
    "axes[1].bar(k_vals, prec_scores, color='lightcoral', alpha=0.7)\n",
    "axes[1].set_xlabel('K (Top-K Results)')\n",
    "axes[1].set_ylabel('Precision Score')\n",
    "axes[1].set_title('Precision@K Scores')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìà MRR (Mean Reciprocal Rank): {retrieval_metrics.get('mrr', 0):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379430e",
   "metadata": {},
   "source": [
    "## Test 2: Factual Consistency & Grounding\n",
    "\n",
    "Evaluates whether generated answers are factually consistent with retrieved sources:\n",
    "- **Claim Verification**: Uses NLI models to verify claims against sources\n",
    "- **Support Ratio**: Fraction of claims supported by retrieved evidence\n",
    "- **Hallucination Detection**: Identifies contradicted or unsupported claims\n",
    "- **Entity Consistency**: Checks factual entity alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ee973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize factual consistency evaluator\n",
    "factual_eval = FactualConsistencyEvaluator()\n",
    "\n",
    "# Sample generated answer and sources\n",
    "generated_answer = \"\"\"\n",
    "Artificial intelligence significantly improves medical diagnosis by analyzing medical images \n",
    "with higher accuracy than human doctors in some cases. AI systems can detect early-stage \n",
    "cancer, diabetic retinopathy, and cardiovascular conditions. Machine learning algorithms \n",
    "process electronic health records to identify patterns and risk factors.\n",
    "\"\"\"\n",
    "\n",
    "source_texts = [\n",
    "    \"AI-powered diagnostic tools have shown remarkable success in detecting diseases like cancer, diabetic retinopathy, and cardiovascular conditions at early stages.\",\n",
    "    \"Studies have demonstrated that AI systems can achieve diagnostic accuracy comparable to or exceeding that of specialist physicians in certain domains.\",\n",
    "    \"Machine learning algorithms can analyze medical images, electronic health records, and genomic data to identify patterns.\"\n",
    "]\n",
    "\n",
    "# Evaluate factual consistency\n",
    "factual_metrics = factual_eval.evaluate_factual_consistency(generated_answer, source_texts)\n",
    "\n",
    "print(\"‚úÖ Factual Consistency Metrics:\")\n",
    "for metric, score in factual_metrics.items():\n",
    "    if isinstance(score, float):\n",
    "        print(f\"   {metric}: {score:.3f}\")\n",
    "    else:\n",
    "        print(f\"   {metric}: {score}\")\n",
    "\n",
    "# Visualize factual consistency results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Support vs contradiction ratio\n",
    "support_ratio = factual_metrics.get('support_ratio', 0)\n",
    "hallucination_ratio = factual_metrics.get('hallucination_ratio', 0)\n",
    "neutral_ratio = 1 - support_ratio - hallucination_ratio\n",
    "\n",
    "labels = ['Supported', 'Neutral', 'Contradicted']\n",
    "sizes = [support_ratio, neutral_ratio, hallucination_ratio]\n",
    "colors = ['lightgreen', 'lightgray', 'lightcoral']\n",
    "\n",
    "ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Claim Support Distribution')\n",
    "\n",
    "# Verification score distribution\n",
    "verification_score = factual_metrics.get('avg_verification_score', 0.5)\n",
    "score_categories = ['Low (0-0.3)', 'Medium (0.3-0.7)', 'High (0.7-1.0)']\n",
    "\n",
    "if verification_score < 0.3:\n",
    "    score_dist = [1, 0, 0]\n",
    "elif verification_score < 0.7:\n",
    "    score_dist = [0, 1, 0]\n",
    "else:\n",
    "    score_dist = [0, 0, 1]\n",
    "\n",
    "ax2.bar(score_categories, score_dist, color=['red', 'orange', 'green'])\n",
    "ax2.set_ylabel('Proportion')\n",
    "ax2.set_title(f'Verification Score Category\\n(Avg: {verification_score:.3f})')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863120da",
   "metadata": {},
   "source": [
    "## Test 3: Multimodal Integration Quality\n",
    "\n",
    "Assesses how well the system integrates visual and textual information:\n",
    "- **Cross-Modal Alignment**: Semantic coherence between text and image sources\n",
    "- **OCR Accuracy**: Quality of text extraction from images\n",
    "- **Visual QA Correctness**: Accuracy of visual question answering\n",
    "- **Source Relevance**: Relevance of both text and image sources to queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multimodal grounding evaluator\n",
    "multimodal_eval = MultimodalGroundingEvaluator()\n",
    "\n",
    "# Sample multimodal query and sources\n",
    "query = \"How do computer vision models analyze medical images?\"\n",
    "\n",
    "text_sources = [\n",
    "    {\n",
    "        'text': 'Computer vision models use convolutional neural networks to analyze medical images for disease detection.',\n",
    "        'score': 0.89\n",
    "    },\n",
    "    {\n",
    "        'text': 'Deep learning algorithms can process X-rays, MRI scans, and CT images with high accuracy.',\n",
    "        'score': 0.82\n",
    "    }\n",
    "]\n",
    "\n",
    "image_sources = [\n",
    "    {\n",
    "        'ocr_text': 'Medical image analysis using CNN architecture for chest X-ray classification',\n",
    "        'caption': 'Diagram showing CNN layers processing medical images',\n",
    "        'score': 0.78\n",
    "    },\n",
    "    {\n",
    "        'ocr_text': 'Deep learning model performance on radiology datasets',\n",
    "        'caption': 'Performance metrics table for medical image AI',\n",
    "        'score': 0.71\n",
    "    }\n",
    "]\n",
    "\n",
    "# Evaluate multimodal grounding\n",
    "multimodal_result = multimodal_eval.evaluate_multimodal_grounding(\n",
    "    query, text_sources, image_sources\n",
    ")\n",
    "\n",
    "print(\"üé≠ Multimodal Integration Metrics:\")\n",
    "print(f\"   Text Source Relevance: {multimodal_result.text_source_relevance:.3f}\")\n",
    "print(f\"   Image Source Relevance: {multimodal_result.image_source_relevance:.3f}\")\n",
    "print(f\"   Cross-Modal Alignment: {multimodal_result.cross_modal_alignment:.3f}\")\n",
    "print(f\"   OCR Accuracy: {multimodal_result.ocr_accuracy:.3f}\")\n",
    "print(f\"   Visual QA Correctness: {multimodal_result.visual_qa_correctness:.3f}\")\n",
    "print(f\"   Overall Grounding Quality: {multimodal_result.grounding_quality}\")\n",
    "\n",
    "# Visualize multimodal metrics\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Radar chart for multimodal metrics\n",
    "metrics = {\n",
    "    'Text Relevance': multimodal_result.text_source_relevance,\n",
    "    'Image Relevance': multimodal_result.image_source_relevance,\n",
    "    'Cross-Modal\\nAlignment': multimodal_result.cross_modal_alignment,\n",
    "    'OCR Accuracy': multimodal_result.ocr_accuracy,\n",
    "    'Visual QA\\nCorrectness': multimodal_result.visual_qa_correctness\n",
    "}\n",
    "\n",
    "labels = list(metrics.keys())\n",
    "values = list(metrics.values())\n",
    "\n",
    "ax1.bar(range(len(labels)), values, color='lightblue', alpha=0.7)\n",
    "ax1.set_xticks(range(len(labels)))\n",
    "ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Multimodal Integration Metrics')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Overall quality distribution\n",
    "quality_levels = ['Poor', 'Fair', 'Good', 'Excellent']\n",
    "quality_scores = [0, 0, 0, 0]\n",
    "\n",
    "quality_map = {'poor': 0, 'fair': 1, 'good': 2, 'excellent': 3}\n",
    "quality_idx = quality_map.get(multimodal_result.grounding_quality.lower(), 1)\n",
    "quality_scores[quality_idx] = 1\n",
    "\n",
    "colors = ['red', 'orange', 'lightgreen', 'green']\n",
    "ax2.bar(quality_levels, quality_scores, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Assessment')\n",
    "ax2.set_title('Overall Grounding Quality')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d08e0",
   "metadata": {},
   "source": [
    "## Comprehensive Evaluation Dashboard\n",
    "\n",
    "Combined view of all evaluation metrics for research reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dddf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive evaluation summary\n",
    "evaluation_summary = {\n",
    "    'Retrieval Quality': {\n",
    "        'NDCG@3': retrieval_metrics.get('ndcg_at_3', 0),\n",
    "        'Precision@3': retrieval_metrics.get('precision_at_3', 0),\n",
    "        'MRR': retrieval_metrics.get('mrr', 0)\n",
    "    },\n",
    "    'Factual Consistency': {\n",
    "        'Support Ratio': factual_metrics.get('support_ratio', 0),\n",
    "        'Avg Verification Score': factual_metrics.get('avg_verification_score', 0),\n",
    "        'Hallucination Ratio': factual_metrics.get('hallucination_ratio', 0)\n",
    "    },\n",
    "    'Multimodal Integration': {\n",
    "        'Text Relevance': multimodal_result.text_source_relevance,\n",
    "        'Image Relevance': multimodal_result.image_source_relevance,\n",
    "        'Cross-Modal Alignment': multimodal_result.cross_modal_alignment\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('FusionGraph Research-Grade Evaluation Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Retrieval Quality Overview\n",
    "ret_metrics = list(evaluation_summary['Retrieval Quality'].keys())\n",
    "ret_scores = list(evaluation_summary['Retrieval Quality'].values())\n",
    "\n",
    "axes[0,0].bar(ret_metrics, ret_scores, color='skyblue', alpha=0.8)\n",
    "axes[0,0].set_title('Retrieval Quality Metrics')\n",
    "axes[0,0].set_ylabel('Score')\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Factual Consistency Overview\n",
    "fact_metrics = list(evaluation_summary['Factual Consistency'].keys())\n",
    "fact_scores = list(evaluation_summary['Factual Consistency'].values())\n",
    "\n",
    "axes[0,1].bar(fact_metrics, fact_scores, color='lightgreen', alpha=0.8)\n",
    "axes[0,1].set_title('Factual Consistency Metrics')\n",
    "axes[0,1].set_ylabel('Score')\n",
    "axes[0,1].set_ylim(0, 1)\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Multimodal Integration Overview\n",
    "mm_metrics = list(evaluation_summary['Multimodal Integration'].keys())\n",
    "mm_scores = list(evaluation_summary['Multimodal Integration'].values())\n",
    "\n",
    "axes[1,0].bar(mm_metrics, mm_scores, color='lightcoral', alpha=0.8)\n",
    "axes[1,0].set_title('Multimodal Integration Metrics')\n",
    "axes[1,0].set_ylabel('Score')\n",
    "axes[1,0].set_ylim(0, 1)\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Overall System Performance Heatmap\n",
    "all_metrics = []\n",
    "all_scores = []\n",
    "\n",
    "for category, metrics in evaluation_summary.items():\n",
    "    for metric, score in metrics.items():\n",
    "        all_metrics.append(f\"{category}\\n{metric}\")\n",
    "        all_scores.append(score)\n",
    "\n",
    "# Create heatmap data\n",
    "heatmap_data = [[score] for score in all_scores]\n",
    "im = axes[1,1].imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "\n",
    "axes[1,1].set_yticks(range(len(all_metrics)))\n",
    "axes[1,1].set_yticklabels(all_metrics, fontsize=8)\n",
    "axes[1,1].set_xticks([])\n",
    "axes[1,1].set_title('Overall Performance Heatmap')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=axes[1,1], shrink=0.8)\n",
    "cbar.set_label('Score', rotation=270, labelpad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nüìä Research-Grade Evaluation Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_df = pd.DataFrame(evaluation_summary).round(3)\n",
    "print(summary_df.to_string())\n",
    "\n",
    "# Calculate overall system score\n",
    "overall_score = sum(all_scores) / len(all_scores)\n",
    "print(f\"\\nüéØ Overall System Performance: {overall_score:.3f}\")\n",
    "\n",
    "if overall_score >= 0.8:\n",
    "    grade = \"A (Excellent)\"\n",
    "elif overall_score >= 0.7:\n",
    "    grade = \"B (Good)\" \n",
    "elif overall_score >= 0.6:\n",
    "    grade = \"C (Satisfactory)\"\n",
    "else:\n",
    "    grade = \"D (Needs Improvement)\"\n",
    "\n",
    "print(f\"üìà Research Grade: {grade}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303eb76b",
   "metadata": {},
   "source": [
    "## Research Applications\n",
    "\n",
    "These quantitative metrics enable:\n",
    "\n",
    "### üìà **Benchmarking & Comparison**\n",
    "- Compare against other RAG systems using standardized metrics\n",
    "- Track performance improvements over time\n",
    "- Validate system reliability for production deployment\n",
    "\n",
    "### üî¨ **Research Publications**\n",
    "- Provides rigorous evaluation methodology for academic papers\n",
    "- Enables reproducible experiments with quantitative results\n",
    "- Supports claims about system effectiveness with evidence\n",
    "\n",
    "### üéØ **System Optimization**\n",
    "- Identify specific areas for improvement (retrieval vs. generation)\n",
    "- A/B test different components with quantitative measures\n",
    "- Optimize hyperparameters based on evaluation metrics\n",
    "\n",
    "### ‚úÖ **Quality Assurance**\n",
    "- Continuous monitoring of system performance\n",
    "- Automated alerts when metrics fall below thresholds\n",
    "- Regression testing for system updates"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
